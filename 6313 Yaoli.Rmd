---
title: "6313Project"
author: "YaoLi"
output:
  html_document: default
  pdf_document: default
NetID: yxl180042
---
```{r}

```


#Primary Token Selection.
In this project, each group will work with three tokens¡¯ data. These will be your primary tokens. To this end, sum the group members¡¯ UTD Ids and take modulo 20. Suppose that your id sum is 123456 whose modulo 20 gives 16. Order the tokens by edge file size on disk and choose the 16th, 17th and 18th biggest token. By this scheme, we will analyze one of the top 20 tokens.If you have selected beautychain1 or beautychain2, please ignore them and use the next token in order. These tokens have failed recently.
My UTDID is 2021431320 whose modulo 20 gives 0,so I take 20th token "networkmcapTX",1th token"networkaragonTX" and 2th token"networkbatTX".

#Question1

##Preprocessing step
Find your tokens and load their data. In each token, there may be outlier amounts which are bigger than the total amount of the token. Locate these extreme outliers, if exist, and filter them out. If there are many of these (>30), investigate how many users are included in these transactions.Update See this news as an example of why we have these outliers: (https://cryptoslate.com/batchoverflow-exploit-creates-trillions-of-ethereum-tokens/)

## MCAP
```{r}
# Load Data
setwd("D:/project/6313")
table<-read.table("networkmcapTX.txt",head=FALSE)
length(table [,1])  # dataset size
```
####Filter Outliter
```{r}
#filter out tokenAmounts > the total circulating token amount
#tokenAmount should be less than decimals*supply(10^18*10^9)
outlier <- 10^27
names(table) <- c("A","B","C","D")
newTable = table[table$D<=outlier,]
names(newTable) <- c("fromNodeID","ttoNodeID","tunixTime","ttokenAmount")
length(newTable [,1]) # dataset size after filterring outliers
```
This means that there has one data is negligible

####Show MCAP Distribution
```{r}
#Buys
buysTable <- as.data.frame.table(table(newTable$ttoNodeID))
names(buysTable) <- c("nodeId","buysFreq")

buysFreqTable <- as.data.frame.table(table(buysTable$buysFreq))
names(buysFreqTable) <- c("buysNumber","count")
plot(buysFreqTable$count~buysFreqTable$buysNumber,main="MCAP Buyers Distribution")

```
```{r}
#Sells
sellsTable <- as.data.frame.table(table(newTable$fromNodeID))
names(sellsTable) <- c("nodeId","sellsFreq")
sellsFreqTable <- as.data.frame.table(table(sellsTable$sellsFreq))
names(sellsFreqTable) <- c("sellsNumber","count")
plot(sellsFreqTable$count~sellsFreqTable$sellsNumber,main="MCAP Sellers Distribution")
```

####Fit Distribution
```{r}
library(fitdistrplus)
fitBuy <- fitdist(buysFreqTable$count,"pois")
fitBuy
```

```{r}
library(fitdistrplus) 
fitSell <- fitdist(sellsFreqTable$count,"pois")
fitSell
```

##Aragon
```{r}
# Load Data
setwd("D:/project/6313")
table1<-read.table("networkaragonTX.txt",head=FALSE)
length(table1 [,1])  # dataset size
```
#### Filter outliter
```{r}
#filter out tokenAmounts > the total circulating token amount
#tokenAmount should be less than decimals*supply(10^18*10^9)
outlier <- 10^27
names(table1) <- c("A","B","C","D")
newTable1 = table1[table1$D<=outlier,]
names(newTable1) <- c("fromNodeID","ttoNodeID","tunixTime","ttokenAmount")
length(newTable1 [,1]) # dataset size after filterring outliers
```
####Show Aragon ditribution
```{r}
#Buys
buysTable1 <- as.data.frame.table(table(newTable1$ttoNodeID))
names(buysTable1) <- c("nodeId","buysFreq")

buysFreqTable1 <- as.data.frame.table(table(buysTable1$buysFreq))
names(buysFreqTable1) <- c("buysNumber","count")
plot(buysFreqTable1$count~buysFreqTable1$buysNumber,main="Aragon Buyers Distribution")
```
```{r}
#Sells
sellsTable1 <- as.data.frame.table(table(newTable1$fromNodeID))
names(sellsTable1) <- c("nodeId","sellsFreq")
sellsFreqTable1 <- as.data.frame.table(table(sellsTable1$sellsFreq))
names(sellsFreqTable1) <- c("sellsNumber","count")
plot(sellsFreqTable1$count~sellsFreqTable1$sellsNumber,main="Aragon Sellers Distribution")
```
#### Fit Distribution
```{r}
library(fitdistrplus)
```
```{r}
fitBuy <- fitdist(buysFreqTable1$count,"pois")
fitBuy
```
```{r}
library(fitdistrplus) 
fitSell <- fitdist(sellsFreqTable1$count,"pois")
fitSell
```

##BAT
```{r}
# Load Data
setwd("D:/project/6313")
table2<-read.table("networkbatTX.txt",head=FALSE)
length(table2 [,1])  # dataset size
```
#### Filter Outliter
```{r}
#filter out tokenAmounts > the total circulating token amount
#tokenAmount should be less than decimals*supply(10^18*10^9)
outlier <- 10^27
names(table2) <- c("A","B","C","D")
newTable2 = table[table2$D<=outlier,]
names(newTable2) <- c("fromNodeID","ttoNodeID","tunixTime","ttokenAmount")
length(newTable2 [,1]) # dataset size after filterring outliers
```
#### Show BAT Distribution
```{r}
#Buys
buysTable2 <- as.data.frame.table(table(newTable2$ttoNodeID))
names(buysTable2) <- c("nodeId","buysFreq")

buysFreqTable2 <- as.data.frame.table(table(buysTable2$buysFreq))
names(buysFreqTable2) <- c("buysNumber","count")
plot(buysFreqTable2$count~buysFreqTable2$buysNumber,main="BAT Buyers Distribution")
```
```{r}
#Sells
sellsTable2 <- as.data.frame.table(table(newTable2$fromNodeID))
names(sellsTable2) <- c("nodeId","sellsFreq")
sellsFreqTable2 <- as.data.frame.table(table(sellsTable2$sellsFreq))
names(sellsFreqTable2) <- c("sellsNumber","count")
plot(sellsFreqTable2$count~sellsFreqTable2$sellsNumber,main="BAT Sellers Distribution")
```
#### Fit Distribution
```{r}
library(fitdistrplus)
fitBuy <- fitdist(buysFreqTable2$count,"pois")
fitBuy
```
```{r}
library(fitdistrplus) 
fitSell <- fitdist(sellsFreqTable2$count,"pois")
fitSell
```







#Question2
This question is similar to the first question. You will find the most active buyers and sellers in each of your three token network, and track them in other tokens. Develop a regression model where ¡°buys¡± of the top K buyers (by number of buys or amount of buys) are regressors, and token price is the outcome. Determine a K value to have the best regression results. This means that you will develop three regression models for three tokens, and K can be different for each model.

```{r}
#Find the most active buyers in MCAP
data=newTable
ActiveBuyMCAP<-sort(table(data$ttoNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyMCAP)
```
```{r}
#Find the most active buyers in Aragon
data=newTable1
ActiveBuyAragon<-sort(table(data$ttoNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyAragon)
```
```{r}
#Find the most active buyers in BAT
data=newTable2
ActiveBuyBAT<-sort(table(data$ttoNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyBAT)
```
```{r}
#Find the most active Sellers in MCAP
data=newTable
ActiveBuyMCAP<-sort(table(data$fromNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyMCAP)
```
```{r}
#Find the most active Sellers in Aragon
data=newTable1
ActiveBuyAragon<-sort(table(data$fromNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyAragon)
```
```{r}
#Find the most active Sellers in BAT
data=newTable2
ActiveBuyBAT<-sort(table(data$fromNodeID))  #Count the frequency of buyers and sort
which.max(ActiveBuyBAT)
```
 So,the most active buyer of MCAP is ttoNodeID=309659,the most active seller of MCAP is fromNodeID=2157432 ;
 the most active buyer of Aragon is ttoNodeID=194319,the most active seller of Aragon is fromNodeID=194319 ;
 the most active buyer of MCAP is ttoNodeID=351141,the most active seller of MCAP is fromNodeID=351141;
 
```{r}
 # number of buys by user id
buys_distribution <- newTable %>% group_by(ttoNodeID) %>% summarise(n = n()) %>% ungroup
# show highest 10 buyers and their number of buys
buys_distribution %>% arrange(-n) %>% head(10)
```
 
 
 
 


